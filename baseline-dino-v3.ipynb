{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":4533,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3325,"modelId":986}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers \n!pip install protobuf==3.20.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:24:34.744182Z","iopub.execute_input":"2025-11-19T05:24:34.744510Z","iopub.status.idle":"2025-11-19T05:24:43.995389Z","shell.execute_reply.started":"2025-11-19T05:24:34.744484Z","shell.execute_reply":"2025-11-19T05:24:43.994463Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nCollecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport os \nimport glob \nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport timm # For pre-trained models\nfrom tqdm import tqdm # For progress bars\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:24:43.997780Z","iopub.execute_input":"2025-11-19T05:24:43.998020Z","iopub.status.idle":"2025-11-19T05:24:54.452333Z","shell.execute_reply.started":"2025-11-19T05:24:43.997995Z","shell.execute_reply":"2025-11-19T05:24:54.451711Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoImageProcessor, AutoModel\n\n# This is the path to the dataset you added in Step 1\n# The exact path may vary slightly based on which dataset you chose.\nMODEL_PATH = '/kaggle/input/dinov2/pytorch/small/1' \n\n# Load the processor from the local files\nprocessor = AutoImageProcessor.from_pretrained(MODEL_PATH)\n\n# Load the model from the local files\nmodel = AutoModel.from_pretrained(MODEL_PATH)\n\nmodel.eval()\nprint(\"DINOv2 model loaded successfully from local files.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:24:54.453003Z","iopub.execute_input":"2025-11-19T05:24:54.453207Z","iopub.status.idle":"2025-11-19T05:25:15.497253Z","shell.execute_reply.started":"2025-11-19T05:24:54.453191Z","shell.execute_reply":"2025-11-19T05:25:15.496483Z"}},"outputs":[{"name":"stderr","text":"2025-11-19 05:24:57.889428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763529898.090604      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763529898.154304      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"DINOv2 model loaded successfully from local files.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- Setup ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:15.498231Z","iopub.execute_input":"2025-11-19T05:25:15.498900Z","iopub.status.idle":"2025-11-19T05:25:15.503648Z","shell.execute_reply.started":"2025-11-19T05:25:15.498871Z","shell.execute_reply":"2025-11-19T05:25:15.502784Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"BASE_PATH='/kaggle/input/csiro-biomass'\ntrain_meta=pd.read_csv(os.path.join(BASE_PATH,'train.csv'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:15.504567Z","iopub.execute_input":"2025-11-19T05:25:15.504821Z","iopub.status.idle":"2025-11-19T05:25:15.532519Z","shell.execute_reply.started":"2025-11-19T05:25:15.504803Z","shell.execute_reply":"2025-11-19T05:25:15.531925Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 1. Pivot the target variables\n# (Pivoting on 'image_id' is a bit cleaner than the full path)\ntargets_df = train_meta.pivot(index='image_path', \n                              columns='target_name', \n                              values='target')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:15.533180Z","iopub.execute_input":"2025-11-19T05:25:15.533394Z","iopub.status.idle":"2025-11-19T05:25:15.562538Z","shell.execute_reply.started":"2025-11-19T05:25:15.533377Z","shell.execute_reply":"2025-11-19T05:25:15.561967Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# 2. Get the unique metadata for each image\n# (State, Species, etc. are repeated, so we just grab the first instance)\nmeta_df = train_meta[[  'image_path', 'Sampling_Date', 'State', 'Species',\n       'Pre_GSHH_NDVI', 'Height_Ave_cm']] \\\n                      .drop_duplicates(subset='image_path') \\\n                      .set_index('image_path')\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:15.564485Z","iopub.execute_input":"2025-11-19T05:25:15.564922Z","iopub.status.idle":"2025-11-19T05:25:15.571826Z","shell.execute_reply.started":"2025-11-19T05:25:15.564901Z","shell.execute_reply":"2025-11-19T05:25:15.571140Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# 3. Join them together\n# This creates one clean row per image with all data\ntrain_df = meta_df.join(targets_df).reset_index()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:15.572622Z","iopub.execute_input":"2025-11-19T05:25:15.572888Z","iopub.status.idle":"2025-11-19T05:25:15.587559Z","shell.execute_reply.started":"2025-11-19T05:25:15.572860Z","shell.execute_reply":"2025-11-19T05:25:15.587012Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Define our 5 output targets\nTARGET_COLS = list(train_df.columns[-5:])\n\n\n# Weights: [Dry_Total_g, Dry_Green_g, Dry_Dead_g, Dry_Clover_g, Dry_Grass_g]\nCOMP_WEIGHTS = torch.tensor([0.5, 0.2, 0.1, 0.1,0.1]).to(device)\n\n# Now train_df is ready!\nprint(train_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:15.588361Z","iopub.execute_input":"2025-11-19T05:25:15.588579Z","iopub.status.idle":"2025-11-19T05:25:15.746536Z","shell.execute_reply.started":"2025-11-19T05:25:15.588564Z","shell.execute_reply":"2025-11-19T05:25:15.745759Z"}},"outputs":[{"name":"stdout","text":"               image_path Sampling_Date State            Species  \\\n0  train/ID1011485656.jpg      2015/9/4   Tas    Ryegrass_Clover   \n1  train/ID1012260530.jpg      2015/4/1   NSW            Lucerne   \n2  train/ID1025234388.jpg      2015/9/1    WA  SubcloverDalkeith   \n3  train/ID1028611175.jpg     2015/5/18   Tas           Ryegrass   \n4  train/ID1035947949.jpg     2015/9/11   Tas           Ryegrass   \n\n   Pre_GSHH_NDVI  Height_Ave_cm  Dry_Clover_g  Dry_Dead_g  Dry_Green_g  \\\n0           0.62         4.6667        0.0000     31.9984      16.2751   \n1           0.55        16.0000        0.0000      0.0000       7.6000   \n2           0.38         1.0000        6.0500      0.0000       0.0000   \n3           0.66         5.0000        0.0000     30.9703      24.2376   \n4           0.54         3.5000        0.4343     23.2239      10.5261   \n\n   Dry_Total_g    GDM_g  \n0      48.2735  16.2750  \n1       7.6000   7.6000  \n2       6.0500   6.0500  \n3      55.2079  24.2376  \n4      34.1844  10.9605  \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"train_df_sample = train_df.iloc[:round(len(train_df)*0.8)]\nval_df_sample = train_df.iloc[round(len(train_df)*0.8):]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:15.747346Z","iopub.execute_input":"2025-11-19T05:25:15.747726Z","iopub.status.idle":"2025-11-19T05:25:15.751854Z","shell.execute_reply.started":"2025-11-19T05:25:15.747696Z","shell.execute_reply":"2025-11-19T05:25:15.751222Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"val_df_sample.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:15.752568Z","iopub.execute_input":"2025-11-19T05:25:15.752843Z","iopub.status.idle":"2025-11-19T05:25:15.767517Z","shell.execute_reply.started":"2025-11-19T05:25:15.752820Z","shell.execute_reply":"2025-11-19T05:25:15.766810Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(71, 11)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# First, you might need to install it:\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2 # Albumentations uses OpenCV\n\n# --- 1. Define your transforms ---\n# These are applied ONLY to the training data\ntrain_transforms = A.Compose([\n    A.HorizontalFlip(p=0.5),     # 50% chance to flip horizontally\n    A.VerticalFlip(p=0.5),       # 50% chance to flip vertically\n    A.RandomRotate90(p=0.5),     # 50% chance to rotate 90 degrees\n    \n    # Color/Distortion\n    A.GaussNoise(p=0.2),\n    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\n    \n    # Normalize (use mean/std of ImageNet)\n    \n    # Convert to PyTorch Tensor\n    ToTensorV2()\n])\n\n# These are for validation. We ONLY resize and normalize. No flipping/distortion.\nval_transforms = A.Compose([\n     A.HorizontalFlip(p=0.5),     # 50% chance to flip horizontally\n    A.VerticalFlip(p=0.5),       # 50% chance to flip vertically\n    A.RandomRotate90(p=0.5),     # 50% chance to rotate 90 degrees\n    \n    # Color/Distortion\n    A.GaussNoise(p=0.2),\n    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\n    \n    ToTensorV2()\n])\n\n\n# --- 2. Update the Custom Dataset Class ---\nclass BiomassDataset(Dataset):\n    def __init__(self, df, target_cols, base_path, processor,model,transforms=None,):\n        self.df = df\n        self.image_paths = [os.path.join(base_path, p) for p in df['image_path']]\n        self.labels = df[target_cols].values.astype(np.float32)\n        self.transforms = transforms # <-- NEW\n        \n        self.model=model\n        self.processor=processor \n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        feature_list=[]\n        # Load image with OpenCV (used by albumentations)\n        # cv2 loads as BGR, so we convert to RGB\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        labels = self.labels[idx]\n\n        image_features=self.processor(image,return_tensors='pt')\n\n        with torch.no_grad():\n            model_out=self.model(**image_features)\n            feature_list.append(model_out.pooler_output.cpu())\n        \n        # Apply transforms (if they exist)\n        if self.transforms:\n            # Albumentations returns a dictionary\n            transformed = self.transforms(image=image)\n            image = transformed['image']\n        \n        # Labels are already a numpy array, just convert to tensor\n        labels = torch.tensor(labels)\n\n        return image, labels,np.array(feature_list)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:15.768399Z","iopub.execute_input":"2025-11-19T05:25:15.768961Z","iopub.status.idle":"2025-11-19T05:25:16.302946Z","shell.execute_reply.started":"2025-11-19T05:25:15.768943Z","shell.execute_reply":"2025-11-19T05:25:16.302315Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_48/2830425699.py:17: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n  A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\n# --- 3. Create Datasets with new transforms ---\ntrain_dataset = BiomassDataset(df=train_df_sample, \n                               target_cols=TARGET_COLS, \n                               base_path=BASE_PATH,\n                               transforms=train_transforms,processor=processor,model=model) # <-- Pass transforms\n\nval_dataset = BiomassDataset(df=val_df_sample, \n                             target_cols=TARGET_COLS, \n                             base_path=BASE_PATH,\n                             transforms=train_transforms,processor=processor,model=model) # <-- Pass transforms\n\n# DataLoaders are created the same way\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:16.303837Z","iopub.execute_input":"2025-11-19T05:25:16.304430Z","iopub.status.idle":"2025-11-19T05:25:16.310876Z","shell.execute_reply.started":"2025-11-19T05:25:16.304403Z","shell.execute_reply":"2025-11-19T05:25:16.310127Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test it\nimages, labels,encodings = next(iter(train_loader))\n\nprint(f\"Image batch shape: {images.shape}\") # Should be [32, 3, 256, 256]\nprint(f\"Labels batch shape: {labels.shape}\") # Should be [32, 5]\nprint(f\"Feature batch shape: {encodings.shape}\")\n# Image batch shape: torch.Size([32, 1000, 2000, 3])\n# Labels batch shape: torch.Size([32, 5])\n# Feature batch shape: torch.Size([32, 1, 1, 384])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:16.311806Z","iopub.execute_input":"2025-11-19T05:25:16.312040Z","iopub.status.idle":"2025-11-19T05:25:32.155157Z","shell.execute_reply.started":"2025-11-19T05:25:16.312024Z","shell.execute_reply":"2025-11-19T05:25:32.154247Z"}},"outputs":[{"name":"stdout","text":"Image batch shape: torch.Size([32, 1000, 2000, 3])\nLabels batch shape: torch.Size([32, 5])\nFeature batch shape: torch.Size([32, 1, 1, 384])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom tqdm import tqdm\n\n# Containers for the full dataset\nall_features = []\nall_labels = []\n\nprint(\"Extracting features from existing pipeline...\")\n\nfor images, labels, encodings in tqdm(train_loader):\n    \n \n    # .view(batch_size, -1) flattens everything after the batch dimension\n    batch_features = encodings.view(encodings.shape[0], -1).cpu().numpy()\n    \n    # 2. Convert labels to numpy\n    batch_labels = labels.cpu().numpy()\n    \n    # 3. Collect\n    all_features.append(batch_features)\n    all_labels.append(batch_labels)\n\n# Concatenate all batches into single numpy arrays\nX_train = np.vstack(all_features)\ny_train = np.vstack(all_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:25:32.156916Z","iopub.execute_input":"2025-11-19T05:25:32.157344Z","iopub.status.idle":"2025-11-19T05:26:08.616001Z","shell.execute_reply.started":"2025-11-19T05:25:32.157310Z","shell.execute_reply":"2025-11-19T05:26:08.615195Z"}},"outputs":[{"name":"stdout","text":"Extracting features from existing pipeline...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9/9 [00:36<00:00,  4.05s/it]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Containers for the full dataset\nall_features_val = []\nall_labels_val = []\nfor images, labels, encodings in tqdm(val_loader):\n    \n \n    # .view(batch_size, -1) flattens everything after the batch dimension\n    batch_features_val = encodings.view(encodings.shape[0], -1).cpu().numpy()\n    \n    # 2. Convert labels to numpy\n    batch_labels_val = labels.cpu().numpy()\n    \n    # 3. Collect\n    all_features_val.append(batch_features_val)\n    all_labels_val.append(batch_labels_val)\n\n# Concatenate all batches into single numpy arrays\nX_val = np.vstack(all_features_val)\ny_val = np.vstack(all_labels_val)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:26:08.617029Z","iopub.execute_input":"2025-11-19T05:26:08.617765Z","iopub.status.idle":"2025-11-19T05:26:23.220154Z","shell.execute_reply.started":"2025-11-19T05:26:08.617738Z","shell.execute_reply":"2025-11-19T05:26:23.219220Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2/2 [00:14<00:00,  7.30s/it]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"\nprint(f\"Final Feature Matrix X: {X_val.shape}\") # Should be (Total_Images, 384)\nprint(f\"Final Label Matrix y:   {y_val.shape}\") # Should be (Total_Images, 5)\n\nprint(f\"Final Feature Matrix X: {X_train.shape}\") # Should be (Total_Images, 384)\nprint(f\"Final Label Matrix y:   {y_train.shape}\") # Should be (Total_Images, 5)\n# Final Feature Matrix X: (71, 384)\n# Final Label Matrix y:   (71, 5)\n# Final Feature Matrix X: (286, 384)\n# Final Label Matrix y:   (286, 5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:26:25.729964Z","iopub.execute_input":"2025-11-19T05:26:25.730592Z","iopub.status.idle":"2025-11-19T05:26:25.735458Z","shell.execute_reply.started":"2025-11-19T05:26:25.730566Z","shell.execute_reply":"2025-11-19T05:26:25.734632Z"}},"outputs":[{"name":"stdout","text":"Final Feature Matrix X: (71, 384)\nFinal Label Matrix y:   (71, 5)\nFinal Feature Matrix X: (286, 384)\nFinal Label Matrix y:   (286, 5)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.model_selection import train_test_split, KFold\n# Import R-squared and MAE for final evaluation\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression \nimport optuna # NEW: Import Optuna\n\n\n# --- 1. DATA SETUP: ---\n\n# GENERATE SYNTHETIC DATA \n\n\n# 1.1. CONCATENATE: Merge the train and validation sets into one full dataset (X_full)\nX_full = np.concatenate([X_train, X_val], axis=0)\ny_full = np.concatenate([y_train, y_val], axis=0)\n\nprint(f\"Combined X_full shape: {X_full.shape}\") \nprint(f\"Combined y_full shape: {y_full.shape}\")\nprint(f\"Number of Target Variables: {y_full.shape[1]}\")\n\n# 1.2. SPLIT: Reserve the final, unseen TEST set (20% of the total data)\nX_cv_tuning, X_test, y_cv_tuning, y_test = train_test_split(\n    X_full, y_full, test_size=0.2, random_state=42\n)\n\nprint(f\"\\nFinal CV Tuning Samples (Used for K-Fold): {X_cv_tuning.shape[0]}\")\nprint(f\"Final Test Samples (Holdout for Evaluation): {X_test.shape[0]}\")\n\n\n# --- 2. SETUP AND UTILITIES ---\n\n# K-Fold strategy for cross-validation (n_splits=3 as requested)\nKF = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# Global variables for tuning data\nX_TUNING = X_cv_tuning\nY_TUNING = y_cv_tuning\n\n\n# --- 3. OPTUNA OBJECTIVE FUNCTIONS (New for Optuna Tuning) ---\n# We define objective functions to be maximized (R2 score).\n\ndef objective_xgb(trial: optuna.Trial):\n    \"\"\"Objective function for XGBoost tuning using K-Fold CV.\"\"\"\n    \n    # 3.1 Define the search space using Optuna's suggestion methods\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 500, 2000, step=500),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 4, 10),\n        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n        'random_state': 42,\n        'n_jobs': -1,\n        # GPU setting for P100\n        'tree_method': 'gpu_hist', \n    }\n    \n    # 3.2 Perform K-Fold Cross-Validation\n    r2_scores = []\n    \n    for train_idx, val_idx in KF.split(X_TUNING, Y_TUNING):\n        X_tr, X_val = X_TUNING[train_idx], X_TUNING[val_idx]\n        y_tr, y_val = Y_TUNING[train_idx], Y_TUNING[val_idx]\n\n        model = MultiOutputRegressor(xgb.XGBRegressor(**param))\n        model.fit(X_tr, y_tr)\n        \n        preds = model.predict(X_val)\n        \n        # Calculate the R2 score for this fold (across all targets)\n        r2 = r2_score(y_val, preds)\n        r2_scores.append(r2)\n        \n    # Optuna maximizes the average score\n    return np.mean(r2_scores)\n\n\ndef objective_lgb(trial: optuna.Trial):\n    \"\"\"Objective function for LightGBM tuning using K-Fold CV.\"\"\"\n    \n    # 3.1 Define the search space\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 500, 2000, step=500),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 15),\n        'subsample': trial.suggest_float('subsample', 0.6, 0.95),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.95),\n        'min_child_samples': trial.suggest_int('min_child_samples', 10, 60),\n        'random_state': 42,\n        'n_jobs': -1,\n        'verbose': -1,\n        # GPU setting for P100\n        'device': 'gpu', \n    }\n    \n    # 3.2 Perform K-Fold Cross-Validation\n    r2_scores = []\n    \n    for train_idx, val_idx in KF.split(X_TUNING, Y_TUNING):\n        X_tr, X_val = X_TUNING[train_idx], X_TUNING[val_idx]\n        y_tr, y_val = Y_TUNING[train_idx], Y_TUNING[val_idx]\n\n        model = MultiOutputRegressor(lgb.LGBMRegressor(**param))\n        model.fit(X_tr, y_tr)\n        \n        preds = model.predict(X_val)\n        \n        # Calculate the R2 score for this fold (across all targets)\n        r2 = r2_score(y_val, preds)\n        r2_scores.append(r2)\n        \n    # Optuna maximizes the average score\n    return np.mean(r2_scores)\n\n\n# --- 4. OPTUNA EXECUTION ---\nprint(\"\\n--- Starting XGBoost Hyperparameter Tuning with Optuna ---\")\n# \nxgb_study = optuna.create_study(direction=\"maximize\", study_name=\"XGBoost_R2_Tuning\")\nxgb_study.optimize(objective_xgb, n_trials=10, show_progress_bar=True) # Increased trials for better search\n\nprint(\"\\nFinished XGBoost Tuning.\")\nprint(f\"Best Optuna R2 Score: {xgb_study.best_value:.4f}\")\nprint(f\"Best XGBoost Parameters: {xgb_study.best_params}\")\n\n# Train the final XGBoost model using the best parameters found\nbest_xgb_params = xgb_study.best_params\nbest_model_xgb = MultiOutputRegressor(xgb.XGBRegressor(\n    **best_xgb_params,\n    random_state=42,\n    n_jobs=-1,\n    tree_method='gpu_hist' # Ensure GPU is used in the final model\n))\nbest_model_xgb.fit(X_TUNING, Y_TUNING)\n\n\nprint(\"\\n--- Starting LightGBM Hyperparameter Tuning with Optuna ---\")\nlgb_study = optuna.create_study(direction=\"maximize\", study_name=\"LGBM_R2_Tuning\")\nlgb_study.optimize(objective_lgb, n_trials=10, show_progress_bar=True)\n\nprint(\"\\nFinished LightGBM Tuning.\")\nprint(f\"Best Optuna R2 Score: {lgb_study.best_value:.4f}\")\nprint(f\"Best LightGBM Parameters: {lgb_study.best_params}\")\n\n# Train the final LightGBM model using the best parameters found\nbest_lgb_params = lgb_study.best_params\nbest_model_lgb = MultiOutputRegressor(lgb.LGBMRegressor(\n    **best_lgb_params,\n    random_state=42,\n    n_jobs=-1,\n    verbose=-1,\n    device='gpu' # Ensure GPU is used in the final model\n))\nbest_model_lgb.fit(X_TUNING, Y_TUNING)\n\n\n# --- 5. WEIGHT TUNING VIA OUT-OF-FOLD (OOF) PREDICTIONS ---\n# The process for OOF generation and meta-learner training remains the same.\n\nprint(\"\\n--- Generating OOF Predictions for Weight Tuning ---\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T05:51:39.112910Z","iopub.execute_input":"2025-11-19T05:51:39.113223Z","iopub.status.idle":"2025-11-19T06:04:08.687327Z","shell.execute_reply.started":"2025-11-19T05:51:39.113195Z","shell.execute_reply":"2025-11-19T06:04:08.686294Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"[I 2025-11-19 05:51:39,133] A new study created in memory with name: XGBoost_R2_Tuning\n","output_type":"stream"},{"name":"stdout","text":"Combined X_full shape: (357, 384)\nCombined y_full shape: (357, 5)\nNumber of Target Variables: 5\n\nFinal CV Tuning Samples (Used for K-Fold): 285\nFinal Test Samples (Holdout for Evaluation): 72\n\n--- Starting XGBoost Hyperparameter Tuning with Optuna ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"602f161e447c446192e8de048edcb3c5"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n/tmp/ipykernel_48/3217660905.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:51:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:51:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:51:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:51:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:51:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:51:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:51:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:51:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:51:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:05] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/tmp/ipykernel_48/3217660905.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n/tmp/ipykernel_48/3217660905.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:52:23,784] Trial 0 finished with value: 0.5178951482997624 and parameters: {'n_estimators': 1500, 'learning_rate': 0.0042614877333521295, 'max_depth': 6, 'subsample': 0.7144731741329484, 'colsample_bytree': 0.6306763902213242, 'reg_alpha': 0.008598098195090133}. Best is trial 0 with value: 0.5178951482997624.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:32] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:35] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/tmp/ipykernel_48/3217660905.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n/tmp/ipykernel_48/3217660905.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:52:46,573] Trial 1 finished with value: 0.5070766875619747 and parameters: {'n_estimators': 1500, 'learning_rate': 0.05419687608736156, 'max_depth': 6, 'subsample': 0.7007786271957113, 'colsample_bytree': 0.8216073038275788, 'reg_alpha': 7.872526227196881}. Best is trial 0 with value: 0.5178951482997624.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:52:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:05] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/tmp/ipykernel_48/3217660905.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n/tmp/ipykernel_48/3217660905.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:53:13,143] Trial 2 finished with value: 0.45605667175509707 and parameters: {'n_estimators': 500, 'learning_rate': 0.003498715936258222, 'max_depth': 10, 'subsample': 0.7933280306511601, 'colsample_bytree': 0.6094114170298421, 'reg_alpha': 0.0011813548011895382}. Best is trial 0 with value: 0.5178951482997624.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:58] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/tmp/ipykernel_48/3217660905.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n/tmp/ipykernel_48/3217660905.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:53:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:53:59,034] Trial 3 finished with value: 0.48145390195229304 and parameters: {'n_estimators': 1500, 'learning_rate': 0.031578084032634235, 'max_depth': 9, 'subsample': 0.6506728976588051, 'colsample_bytree': 0.7863706336119097, 'reg_alpha': 0.0035381310287971967}. Best is trial 0 with value: 0.5178951482997624.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:32] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:54:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:55:11,308] Trial 4 finished with value: 0.4541461937598184 and parameters: {'n_estimators': 1500, 'learning_rate': 0.0012678215515453779, 'max_depth': 8, 'subsample': 0.9065074459311449, 'colsample_bytree': 0.6900514872792954, 'reg_alpha': 0.8894169612054732}. Best is trial 0 with value: 0.5178951482997624.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n/tmp/ipykernel_48/3217660905.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:55:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:56:08,769] Trial 5 finished with value: 0.49199147296192525 and parameters: {'n_estimators': 1500, 'learning_rate': 0.01409717245126502, 'max_depth': 8, 'subsample': 0.62318570992849, 'colsample_bytree': 0.9077609674603228, 'reg_alpha': 0.00025283601985323883}. Best is trial 0 with value: 0.5178951482997624.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n/tmp/ipykernel_48/3217660905.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/tmp/ipykernel_48/3217660905.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n/tmp/ipykernel_48/3217660905.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:56:26,791] Trial 6 finished with value: 0.49608968891326227 and parameters: {'n_estimators': 1000, 'learning_rate': 0.07273053045313024, 'max_depth': 6, 'subsample': 0.7832775387925266, 'colsample_bytree': 0.9338287262222742, 'reg_alpha': 0.014822330592685565}. Best is trial 0 with value: 0.5178951482997624.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:32] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:44] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:48] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:56] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/tmp/ipykernel_48/3217660905.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n/tmp/ipykernel_48/3217660905.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:56:56,999] Trial 7 finished with value: 0.5108005084475166 and parameters: {'n_estimators': 1000, 'learning_rate': 0.007077538144414412, 'max_depth': 6, 'subsample': 0.6328550269183458, 'colsample_bytree': 0.8682351727008533, 'reg_alpha': 0.006352072338308333}. Best is trial 0 with value: 0.5178951482997624.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:56:59] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/tmp/ipykernel_48/3217660905.py:55: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n/tmp/ipykernel_48/3217660905.py:59: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:33] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:57:33,717] Trial 8 finished with value: 0.5033411580274536 and parameters: {'n_estimators': 1000, 'learning_rate': 0.027377317591330916, 'max_depth': 8, 'subsample': 0.6167554184829165, 'colsample_bytree': 0.7021584927551735, 'reg_alpha': 0.004184284321820539}. Best is trial 0 with value: 0.5178951482997624.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:50] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:57:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:00] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:04] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:58:25,060] Trial 9 finished with value: 0.5039019327580735 and parameters: {'n_estimators': 2000, 'learning_rate': 0.0014596443402862067, 'max_depth': 5, 'subsample': 0.6246635700025478, 'colsample_bytree': 0.7982125409922242, 'reg_alpha': 0.031360298024176905}. Best is trial 0 with value: 0.5178951482997624.\n\nFinished XGBoost Tuning.\nBest Optuna R2 Score: 0.5179\nBest XGBoost Parameters: {'n_estimators': 1500, 'learning_rate': 0.0042614877333521295, 'max_depth': 6, 'subsample': 0.7144731741329484, 'colsample_bytree': 0.6306763902213242, 'reg_alpha': 0.008598098195090133}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:32] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:35] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [05:58:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n[I 2025-11-19 05:58:43,092] A new study created in memory with name: LGBM_R2_Tuning\n","output_type":"stream"},{"name":"stdout","text":"\n--- Starting LightGBM Hyperparameter Tuning with Optuna ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9f9dde88c54bb4bce4ece831bc7211"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:59:04,338] Trial 0 finished with value: 0.46126151575191116 and parameters: {'n_estimators': 1500, 'learning_rate': 0.005189609797830659, 'num_leaves': 62, 'max_depth': 13, 'subsample': 0.7573417830396347, 'colsample_bytree': 0.9243101806003199, 'min_child_samples': 55}. Best is trial 0 with value: 0.46126151575191116.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:59:24,325] Trial 1 finished with value: 0.4549968027308953 and parameters: {'n_estimators': 1500, 'learning_rate': 0.030667665869664563, 'num_leaves': 34, 'max_depth': 14, 'subsample': 0.7881254145621314, 'colsample_bytree': 0.7408931209749557, 'min_child_samples': 42}. Best is trial 0 with value: 0.46126151575191116.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:59:47,332] Trial 2 finished with value: 0.4614238100003023 and parameters: {'n_estimators': 1500, 'learning_rate': 0.050087519409197215, 'num_leaves': 32, 'max_depth': 9, 'subsample': 0.8062705375659429, 'colsample_bytree': 0.8902096062711022, 'min_child_samples': 40}. Best is trial 2 with value: 0.4614238100003023.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 05:59:53,652] Trial 3 finished with value: 0.43714051622900946 and parameters: {'n_estimators': 500, 'learning_rate': 0.009184575391376992, 'num_leaves': 37, 'max_depth': 13, 'subsample': 0.7521875182542062, 'colsample_bytree': 0.7221568146328383, 'min_child_samples': 58}. Best is trial 2 with value: 0.4614238100003023.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 06:00:34,942] Trial 4 finished with value: 0.4535622101953302 and parameters: {'n_estimators': 2000, 'learning_rate': 0.03256732914073932, 'num_leaves': 92, 'max_depth': 9, 'subsample': 0.7093462328573975, 'colsample_bytree': 0.8168299289418148, 'min_child_samples': 30}. Best is trial 2 with value: 0.4614238100003023.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 06:00:53,972] Trial 5 finished with value: 0.4577849664185112 and parameters: {'n_estimators': 1000, 'learning_rate': 0.003825889868105876, 'num_leaves': 83, 'max_depth': 14, 'subsample': 0.8255093294463499, 'colsample_bytree': 0.7850207428685527, 'min_child_samples': 31}. Best is trial 2 with value: 0.4614238100003023.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 06:01:10,250] Trial 6 finished with value: 0.443484697775195 and parameters: {'n_estimators': 1500, 'learning_rate': 0.06590594950616549, 'num_leaves': 33, 'max_depth': 12, 'subsample': 0.9189206300065661, 'colsample_bytree': 0.8062935578296611, 'min_child_samples': 51}. Best is trial 2 with value: 0.4614238100003023.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 06:02:30,429] Trial 7 finished with value: 0.4728692712448508 and parameters: {'n_estimators': 2000, 'learning_rate': 0.00576235388506513, 'num_leaves': 29, 'max_depth': 8, 'subsample': 0.7489070946327819, 'colsample_bytree': 0.6841693879601901, 'min_child_samples': 12}. Best is trial 7 with value: 0.4728692712448508.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 06:02:56,657] Trial 8 finished with value: 0.45884657206192125 and parameters: {'n_estimators': 2000, 'learning_rate': 0.019304126980988306, 'num_leaves': 39, 'max_depth': 6, 'subsample': 0.866416996939983, 'colsample_bytree': 0.7530514212674109, 'min_child_samples': 42}. Best is trial 7 with value: 0.4728692712448508.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_48/3217660905.py:92: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n  'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n","output_type":"stream"},{"name":"stdout","text":"[I 2025-11-19 06:03:31,260] Trial 9 finished with value: 0.4629460942397725 and parameters: {'n_estimators': 2000, 'learning_rate': 0.032477709987925145, 'num_leaves': 79, 'max_depth': 14, 'subsample': 0.7353455843460054, 'colsample_bytree': 0.8074606237414603, 'min_child_samples': 35}. Best is trial 7 with value: 0.4728692712448508.\n\nFinished LightGBM Tuning.\nBest Optuna R2 Score: 0.4729\nBest LightGBM Parameters: {'n_estimators': 2000, 'learning_rate': 0.00576235388506513, 'num_leaves': 29, 'max_depth': 8, 'subsample': 0.7489070946327819, 'colsample_bytree': 0.6841693879601901, 'min_child_samples': 12}\n\n--- Generating OOF Predictions for Weight Tuning ---\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/3217660905.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;31m# 5.1 Generate OOF predictions using the best hyperparameters found above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m \u001b[0moof_preds_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_xgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_TUNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_TUNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0moof_preds_lgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_oof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_lgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_TUNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_TUNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_48/3217660905.py\u001b[0m in \u001b[0;36mgenerate_oof\u001b[0;34m(model, X_train, y_train, kf_splitter)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# We clone the model to ensure a fresh training for each fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mfold_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiOutputRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mfold_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'MultiOutputRegressor' object has no attribute 'estimator_'"],"ename":"AttributeError","evalue":"'MultiOutputRegressor' object has no attribute 'estimator_'","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"xgb_study.best_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:05:48.778038Z","iopub.execute_input":"2025-11-19T06:05:48.778611Z","iopub.status.idle":"2025-11-19T06:05:48.783639Z","shell.execute_reply.started":"2025-11-19T06:05:48.778587Z","shell.execute_reply":"2025-11-19T06:05:48.782895Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'n_estimators': 1500,\n 'learning_rate': 0.0042614877333521295,\n 'max_depth': 6,\n 'subsample': 0.7144731741329484,\n 'colsample_bytree': 0.6306763902213242,\n 'reg_alpha': 0.008598098195090133}"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# The generate_oof function remains valid for use with the final Optuna-tuned models\ndef generate_oof(model_param, X_train, y_train, kf_splitter):\n    \"\"\"Generates out-of-fold predictions for the model.\"\"\"\n    oof_preds = np.zeros_like(y_train)\n    \n    # Iterate through the K-Fold splits\n    for fold, (train_idx, val_idx) in enumerate(kf_splitter.split(X_train, y_train)):\n        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n        \n        # We clone the model to ensure a fresh training for each fold\n        fold_model = MultiOutputRegressor(model_param.estimator) \n        fold_model.fit(X_tr, y_tr)\n        \n        # Predict on the validation subset (the \"out-of-fold\" data)\n        oof_preds[val_idx] = fold_model.predict(X_val)\n        \n    return oof_preds\n\n# 5.1 Generate OOF predictions using the best hyperparameters found above\noof_preds_xgb = generate_oof(best_model_xgb, X_TUNING, Y_TUNING, KF)\noof_preds_lgb = generate_oof(best_model_lgb, X_TUNING, Y_TUNING, KF)\n\n# 5.2 Train a Meta-Learner (Linear Regression) to find the optimal blending coefficients (weights)\n\nprint(\"\\n--- Training Linear Meta-Learner for Optimal Blending Weights ---\")\nn_targets = Y_TUNING.shape[1]\nmeta_learners = []\noptimal_weights = []\n\nfor i in range(n_targets):\n    # Prepare OOF features: [XGB_Pred_Target_i, LGBM_Pred_Target_i]\n    X_meta_i = np.column_stack([oof_preds_xgb[:, i], oof_preds_lgb[:, i]])\n    y_meta_i = Y_TUNING[:, i]\n\n    # Train a meta-model for the current output target\n    meta_model = LinearRegression()\n    meta_model.fit(X_meta_i, y_meta_i)\n    meta_learners.append(meta_model)\n\n    # Store the weights (coefficients)\n    weights = meta_model.coef_\n    optimal_weights.append(weights)\n    \n    # Print the learned weights for visibility\n    print(f\"Target {i+1} Optimal Weights: XGB={weights[0]:.3f}, LGBM={weights[1]:.3f}\")\n\n\n# --- 6. FINAL ENSEMBLE EVALUATION WITH OPTIMAL BLENDING ---\n\nprint(\"\\n--- Final Ensemble Prediction on Holdout Test Set ---\")\n\n# 6.1 Predict on the unseen X_test set using the best base models\npreds_xgb_tuned = best_model_xgb.predict(X_test)\npreds_lgb_tuned = best_model_lgb.predict(X_test)\n\n# 6.2 Combine predictions using the learned Meta-Learners\nensemble_preds_optimal = np.zeros_like(y_test)\n\nfor i in range(n_targets):\n    # Prepare test features for the meta-model\n    X_test_meta_i = np.column_stack([preds_xgb_tuned[:, i], preds_lgb_tuned[:, i]])\n    \n    # Predict using the trained meta-learner for the current output target\n    ensemble_preds_optimal[:, i] = meta_learners[i].predict(X_test_meta_i)\n\n# POST-PROCESSING: Biomass cannot be negative\nensemble_preds_optimal = np.maximum(ensemble_preds_optimal, 0)\n\n# Calculate Scores (both R2 and MAE)\ntest_r2_optimal = r2_score(y_test, ensemble_preds_optimal)\ntest_mae_optimal = mean_absolute_error(y_test, ensemble_preds_optimal)\n\nprint(f\"\\n✅ Final Ensemble Test R2 Score (Optimal Blending): {test_r2_optimal:.4f}\")\nprint(f\"✅ Final Ensemble Test MAE (Optimal Blending): {test_mae_optimal:.4f}\")\n\n# For comparison, re-calculate 50/50 simple average score\nsimple_ensemble_preds = (0.5 * preds_xgb_tuned) + (0.5 * preds_lgb_tuned)\nsimple_ensemble_preds = np.maximum(simple_ensemble_preds, 0)\ntest_r2_simple = r2_score(y_test, simple_ensemble_preds)\ntest_mae_simple = mean_absolute_error(y_test, simple_ensemble_preds)\n\nprint(f\"\\n   For comparison: Simple Average (50/50) Test R2: {test_r2_simple:.4f}\")\nprint(f\"   For comparison: Simple Average (50/50) Test MAE: {test_mae_simple:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:11:11.368118Z","iopub.execute_input":"2025-11-19T06:11:11.368666Z","iopub.status.idle":"2025-11-19T06:13:17.771882Z","shell.execute_reply.started":"2025-11-19T06:11:11.368640Z","shell.execute_reply":"2025-11-19T06:13:17.771344Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:20] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:22] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:43] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:11:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"\n--- Training Linear Meta-Learner for Optimal Blending Weights ---\nTarget 1 Optimal Weights: XGB=1.267, LGBM=-0.217\nTarget 2 Optimal Weights: XGB=0.686, LGBM=0.262\nTarget 3 Optimal Weights: XGB=0.985, LGBM=0.211\nTarget 4 Optimal Weights: XGB=1.137, LGBM=0.003\nTarget 5 Optimal Weights: XGB=0.822, LGBM=0.372\n\n--- Final Ensemble Prediction on Holdout Test Set ---\n\n✅ Final Ensemble Test R2 Score (Optimal Blending): 0.5837\n✅ Final Ensemble Test MAE (Optimal Blending): 8.1719\n\n   For comparison: Simple Average (50/50) Test R2: 0.5676\n   For comparison: Simple Average (50/50) Test MAE: 8.2994\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [06:13:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"best_model_xgb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:15:17.487662Z","iopub.execute_input":"2025-11-19T06:15:17.488238Z","iopub.status.idle":"2025-11-19T06:15:17.498289Z","shell.execute_reply.started":"2025-11-19T06:15:17.488218Z","shell.execute_reply":"2025-11-19T06:15:17.497632Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"MultiOutputRegressor(estimator=XGBRegressor(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=0.6306763902213242,\n                                            device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric=None,\n                                            feature_types=None, gamma=None,\n                                            grow_policy=None,\n                                            importance_type=None,\n                                            interaction_constraints=None,\n                                            learning_rate=0.0042614877333521295,\n                                            max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=6,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=1500, n_jobs=-1,\n                                            num_parallel_tree=None,\n                                            random_state=42, ...))","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiOutputRegressor(estimator=XGBRegressor(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=0.6306763902213242,\n                                            device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric=None,\n                                            feature_types=None, gamma=None,\n                                            grow_policy=None,\n                                            importance_type=None,\n                                            interaction_constraints=None,\n                                            learning_rate=0.0042614877333521295,\n                                            max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=6,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=1500, n_jobs=-1,\n                                            num_parallel_tree=None,\n                                            random_state=42, ...))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiOutputRegressor</label><div class=\"sk-toggleable__content\"><pre>MultiOutputRegressor(estimator=XGBRegressor(base_score=None, booster=None,\n                                            callbacks=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=0.6306763902213242,\n                                            device=None,\n                                            early_stopping_rounds=None,\n                                            enable_categorical=False,\n                                            eval_metric=None,\n                                            feature_types=None, gamma=None,\n                                            grow_policy=None,\n                                            importance_type=None,\n                                            interaction_constraints=None,\n                                            learning_rate=0.0042614877333521295,\n                                            max_bin=None,\n                                            max_cat_threshold=None,\n                                            max_cat_to_onehot=None,\n                                            max_delta_step=None, max_depth=6,\n                                            max_leaves=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            multi_strategy=None,\n                                            n_estimators=1500, n_jobs=-1,\n                                            num_parallel_tree=None,\n                                            random_state=42, ...))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.6306763902213242, device=None,\n             early_stopping_rounds=None, enable_categorical=False,\n             eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=0.0042614877333521295, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=6, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1500, n_jobs=-1,\n             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=0.6306763902213242, device=None,\n             early_stopping_rounds=None, enable_categorical=False,\n             eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=0.0042614877333521295, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=6, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=1500, n_jobs=-1,\n             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"import joblib\nimport os\n\n# Create a folder for models if it doesn't exist\nos.makedirs(\"models\", exist_ok=True)\n\nprint(\"Saving models...\")\n\n# 1. Save the XGBoost Ensemble\njoblib.dump(best_model_xgb, \"models/xgboost_ensemble.pkl\")\n\n# 2. Save the LightGBM Ensemble\njoblib.dump(best_model_lgb, \"models/lightgbm_ensemble.pkl\")\n\nprint(\"Models saved successfully in the 'models' folder!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:15:45.792151Z","iopub.execute_input":"2025-11-19T06:15:45.792461Z","iopub.status.idle":"2025-11-19T06:15:46.293843Z","shell.execute_reply.started":"2025-11-19T06:15:45.792441Z","shell.execute_reply":"2025-11-19T06:15:46.292954Z"}},"outputs":[{"name":"stdout","text":"Saving models...\nModels saved successfully in the 'models' folder!\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
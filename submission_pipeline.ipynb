{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":13776976,"sourceType":"datasetVersion","datasetId":8768931},{"sourceId":277095319,"sourceType":"kernelVersion"},{"sourceId":279819146,"sourceType":"kernelVersion"},{"sourceId":4534,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3326,"modelId":986}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install transformers \n# !pip install protobuf==3.20.3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\n\n\n# Add the path to the wheel file\n# Note: Update 'protobuf-downloader' to match the name of your notebook from Step 2\n!pip install /kaggle/input/my-proto-wheel/protobuf-3.20.3-py2.py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:55:58.017500Z","iopub.execute_input":"2025-11-19T06:55:58.017887Z","iopub.status.idle":"2025-11-19T06:56:02.219656Z","shell.execute_reply.started":"2025-11-19T06:55:58.017859Z","shell.execute_reply":"2025-11-19T06:56:02.218225Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/my-proto-wheel/protobuf-3.20.3-py2.py3-none-any.whl\nprotobuf is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoImageProcessor, AutoModel\n\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:56:02.222180Z","iopub.execute_input":"2025-11-19T06:56:02.222559Z","iopub.status.idle":"2025-11-19T06:56:02.229369Z","shell.execute_reply.started":"2025-11-19T06:56:02.222523Z","shell.execute_reply":"2025-11-19T06:56:02.228301Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings('ignore')\n\n# --- 1. Setup & Config ---\nBASE_PATH = '/kaggle/input/csiro-biomass'\nTEST_IMAGE_PATH = os.path.join(BASE_PATH)\nTEST_META_PATH = os.path.join(BASE_PATH, 'test.csv')\n # Path to your saved model\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\nimport cv2 \nimport pandas as pd\nimport numpy as np\nimport torch\nimport os\nimport joblib # For loading XGB/LGBM\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nMODEL_PATH = '/kaggle/input/dinov2/pytorch/base/1' \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# --- LOAD DINO MODEL ---\nprint(\"Loading DINOv2...\")\nprocessor = AutoImageProcessor.from_pretrained(MODEL_PATH)\ndino_model = AutoModel.from_pretrained(MODEL_PATH)\ndino_model.to(device)\ndino_model.eval()\n\n# --- LOAD ML MODELS ---\nprint(\"Loading Ensemble Models...\")\n# Make sure these paths match where you saved them\nxgb_model = joblib.load(\"/kaggle/input/baseline-dino-v3/models/lightgbm_ensemble.pkl\")\nlgb_model = joblib.load(\"/kaggle/input/baseline-dino-v3/models/xgboost_ensemble.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:56:05.461002Z","iopub.execute_input":"2025-11-19T06:56:05.461730Z","iopub.status.idle":"2025-11-19T06:56:08.721305Z","shell.execute_reply.started":"2025-11-19T06:56:05.461694Z","shell.execute_reply":"2025-11-19T06:56:08.719925Z"}},"outputs":[{"name":"stdout","text":"Loading DINOv2...\nLoading Ensemble Models...\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# --- !! IMPORTANT: Use the CORRECT target columns !! ---\n# These names are used to create the final submission file\nTARGET_COLS =['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n\nIMG_SIZE = 256 # Must be the same size you trained with","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:56:08.725631Z","iopub.execute_input":"2025-11-19T06:56:08.725988Z","iopub.status.idle":"2025-11-19T06:56:08.731302Z","shell.execute_reply.started":"2025-11-19T06:56:08.725960Z","shell.execute_reply":"2025-11-19T06:56:08.730098Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class BiomassTestDataset(Dataset):\n    def __init__(self, df, base_path, processor, model):\n        self.df = df\n        self.image_paths = [os.path.join(base_path, p) for p in df['image_path']]\n        self.processor = processor\n        self.model = model\n        self.device = model.device\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        \n        # 1. Load Image\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        # 2. Process Image for DINO\n        inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n        \n        # 3. Extract Features\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            # Get the pooler_output and move to CPU immediately\n            # Output shape from DINO is [1, 384]\n            features = outputs.pooler_output.cpu()\n            \n        # Return the flattened feature vector [384,]\n        return features.squeeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:56:10.100361Z","iopub.execute_input":"2025-11-19T06:56:10.101354Z","iopub.status.idle":"2025-11-19T06:56:10.109641Z","shell.execute_reply.started":"2025-11-19T06:56:10.101321Z","shell.execute_reply":"2025-11-19T06:56:10.108087Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# --- Prepare DataLoader ---\ntest_df = pd.read_csv(TEST_META_PATH)\n\ntest_dataset = BiomassTestDataset(\n    df=test_df,\n    base_path=TEST_IMAGE_PATH,\n    processor=processor,\n    model=dino_model\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=32, \n    shuffle=False,\n    num_workers=2\n)\n\n# --- Run Feature Extraction ---\nall_features = []\n\nprint(\"Extracting features from Test set...\")\nwith torch.no_grad():\n    for feature_batch in tqdm(test_loader, desc=\"Extracting\"):\n        # feature_batch shape is [32, 384]\n        all_features.append(feature_batch.numpy())\n\n# Create the Final Feature Matrix\nX_test = np.vstack(all_features)\nprint(f\"Test Feature Shape: {X_test.shape}\") # Should be (N_test_images, 384)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:56:11.395164Z","iopub.execute_input":"2025-11-19T06:56:11.395545Z","iopub.status.idle":"2025-11-19T06:56:16.617717Z","shell.execute_reply.started":"2025-11-19T06:56:11.395515Z","shell.execute_reply":"2025-11-19T06:56:16.616178Z"}},"outputs":[{"name":"stdout","text":"Extracting features from Test set...\n","output_type":"stream"},{"name":"stderr","text":"Extracting: 100%|██████████| 1/1 [00:05<00:00,  5.18s/it]","output_type":"stream"},{"name":"stdout","text":"Test Feature Shape: (5, 768)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# print(\"Predicting with Ensemble...\")\n\n# # 1. Predict with XGBoost\n# preds_xgb = xgb_model.predict(X_test)\n\n# # 2. Predict with LightGBM\n# preds_lgb = lgb_model.predict(X_test)\n\n# # 3. Weighted Average (50/50)\n# ensemble_preds = (0.5 * preds_xgb) + (0.5 * preds_lgb)\n\n# # 4. Clip negative values (Biomass cannot be < 0)\n# ensemble_preds = np.maximum(ensemble_preds, 0)\n\n# print(f\"Final Predictions Shape: {ensemble_preds.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T11:34:09.140753Z","iopub.execute_input":"2025-11-18T11:34:09.141171Z","iopub.status.idle":"2025-11-18T11:34:09.170884Z","shell.execute_reply.started":"2025-11-18T11:34:09.141138Z","shell.execute_reply":"2025-11-18T11:34:09.168452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N_TARGETS=len(TARGET_COLS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:57:50.230358Z","iopub.execute_input":"2025-11-19T06:57:50.230768Z","iopub.status.idle":"2025-11-19T06:57:50.236280Z","shell.execute_reply.started":"2025-11-19T06:57:50.230742Z","shell.execute_reply":"2025-11-19T06:57:50.235106Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"META_LEARNER_COEFFS = np.array([\n    [1.267, -0.217],  # Target 1\n    [0.686, 0.262],   # Target 2\n    [0.985, 0.211],   # Target 3\n    [1.137, 0.003],   # Target 4\n    [0.822, 0.372]    # Target 5\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:58:19.895167Z","iopub.execute_input":"2025-11-19T06:58:19.895504Z","iopub.status.idle":"2025-11-19T06:58:19.901307Z","shell.execute_reply.started":"2025-11-19T06:58:19.895480Z","shell.execute_reply":"2025-11-19T06:58:19.900137Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# --- PREDICTION STEP: APPLYING OPTIMAL WEIGHTS ---\n\nprint(\"\\nPredicting with Optimal Ensemble Blend...\")\n\n# 1. Predict with XGBoost\npreds_xgb = xgb_model.predict(X_test)\n\n# 2. Predict with LightGBM\npreds_lgb = lgb_model.predict(X_test)\n\n# 3. Optimal Weighted Average (Target-Specific Stacking)\nensemble_preds_optimal = np.zeros_like(preds_xgb)\n\n# Apply the learned optimal weights (coefficients) column by column\nfor i in range(N_TARGETS):\n    xgb_weight = META_LEARNER_COEFFS[i, 0]\n    lgbm_weight = META_LEARNER_COEFFS[i, 1]\n    \n    # Blend the predictions for the current target (column i)\n    ensemble_preds_optimal[:, i] = (\n        xgb_weight * preds_xgb[:, i] + \n        lgbm_weight * preds_lgb[:, i]\n    )\n\n# 4. Clip negative values (Biomass cannot be < 0)\nensemble_preds = np.maximum(ensemble_preds_optimal, 0)\n\nprint(f\"Final Predictions Shape: {ensemble_preds.shape}\")\n# --- Create Submission File ---\nprint(\"\\nCreating submission file...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:58:21.985096Z","iopub.execute_input":"2025-11-19T06:58:21.985459Z","iopub.status.idle":"2025-11-19T06:58:22.009677Z","shell.execute_reply.started":"2025-11-19T06:58:21.985410Z","shell.execute_reply":"2025-11-19T06:58:22.008302Z"}},"outputs":[{"name":"stdout","text":"\nPredicting with Optimal Ensemble Blend...\nFinal Predictions Shape: (5, 5)\n\nCreating submission file...\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"\n# --- Create Submission File ---\nprint(\"Creating submission file...\")\n\n# 1. Create a \"wide\" DataFrame with predictions\npred_df = pd.DataFrame(ensemble_preds, columns=TARGET_COLS)\n\n# 2. Add the 'image_path' from the original test_df\nsubmission_df = test_df[['image_path']].copy()\nsubmission_df = pd.concat([submission_df, pred_df], axis=1)\n\n# 3. \"Melt\" the DataFrame from \"wide\" to \"long\" format\nsubmission_df_long = submission_df.melt(\n    id_vars=['image_path'],\n    value_vars=TARGET_COLS,\n    var_name='target_name',\n    value_name='target'\n)\n\n# 4. Create the 'sample_id'\n# Clean up the filename (remove path and extension)\nsubmission_df_long['image_name_base'] = submission_df_long['image_path'].apply(\n    lambda p: os.path.splitext(os.path.basename(p))[0]\n)\n\n# Create ID: image_base + __ + target_name\nsubmission_df_long['sample_id'] = submission_df_long['image_name_base'] + '__' + submission_df_long['target_name']\n\n# 5. Filter duplicates and Select Columns\nfinal_submission = submission_df_long[['sample_id', 'target']]\nfinal_submission = final_submission.drop_duplicates(subset=['sample_id'])\n\n# 6. Save\nfinal_submission.to_csv('submission.csv', index=False)\n\nprint(\"submission.csv created successfully!\")\nprint(final_submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T06:58:32.085875Z","iopub.execute_input":"2025-11-19T06:58:32.086229Z","iopub.status.idle":"2025-11-19T06:58:32.138411Z","shell.execute_reply.started":"2025-11-19T06:58:32.086204Z","shell.execute_reply":"2025-11-19T06:58:32.137421Z"}},"outputs":[{"name":"stdout","text":"Creating submission file...\nsubmission.csv created successfully!\n                     sample_id     target\n0   ID1001187975__Dry_Clover_g   2.219094\n5     ID1001187975__Dry_Dead_g  22.625428\n10   ID1001187975__Dry_Green_g  31.627779\n15   ID1001187975__Dry_Total_g  65.787762\n20         ID1001187975__GDM_g  33.710126\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
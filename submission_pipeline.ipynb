{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32048da3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:38:45.279698Z",
     "iopub.status.busy": "2025-11-19T11:38:45.279368Z",
     "iopub.status.idle": "2025-11-19T11:38:45.284923Z",
     "shell.execute_reply": "2025-11-19T11:38:45.283792Z"
    },
    "papermill": {
     "duration": 0.012916,
     "end_time": "2025-11-19T11:38:45.286745",
     "exception": false,
     "start_time": "2025-11-19T11:38:45.273829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers \n",
    "# !pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170a9066",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:38:45.295821Z",
     "iopub.status.busy": "2025-11-19T11:38:45.294919Z",
     "iopub.status.idle": "2025-11-19T11:38:52.856596Z",
     "shell.execute_reply": "2025-11-19T11:38:52.855427Z"
    },
    "papermill": {
     "duration": 7.568778,
     "end_time": "2025-11-19T11:38:52.859283",
     "exception": false,
     "start_time": "2025-11-19T11:38:45.290505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/my-proto-wheel/protobuf-3.20.3-py2.py3-none-any.whl\r\n",
      "Installing collected packages: protobuf\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 6.33.0\r\n",
      "    Uninstalling protobuf-6.33.0:\r\n",
      "      Successfully uninstalled protobuf-6.33.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "onnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "a2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "ray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\r\n",
      "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\r\n",
      "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "# Add the path to the wheel file\n",
    "# Note: Update 'protobuf-downloader' to match the name of your notebook from Step 2\n",
    "!pip install /kaggle/input/my-proto-wheel/protobuf-3.20.3-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca01f0ff",
   "metadata": {
    "papermill": {
     "duration": 0.003322,
     "end_time": "2025-11-19T11:38:52.866303",
     "exception": false,
     "start_time": "2025-11-19T11:38:52.862981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d03b6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:38:52.875746Z",
     "iopub.status.busy": "2025-11-19T11:38:52.874938Z",
     "iopub.status.idle": "2025-11-19T11:39:41.754114Z",
     "shell.execute_reply": "2025-11-19T11:39:41.752503Z"
    },
    "papermill": {
     "duration": 48.886148,
     "end_time": "2025-11-19T11:39:41.755962",
     "exception": false,
     "start_time": "2025-11-19T11:38:52.869814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 11:39:15.584733: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763552355.867555      13 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1763552355.959197      13 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6afa52cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:39:41.765366Z",
     "iopub.status.busy": "2025-11-19T11:39:41.764618Z",
     "iopub.status.idle": "2025-11-19T11:40:25.220320Z",
     "shell.execute_reply": "2025-11-19T11:40:25.218909Z"
    },
    "papermill": {
     "duration": 43.46263,
     "end_time": "2025-11-19T11:40:25.222309",
     "exception": false,
     "start_time": "2025-11-19T11:39:41.759679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2...\n",
      "Loading Ensemble Models...\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 1. Setup & Config ---\n",
    "BASE_PATH = '/kaggle/input/csiro-biomass'\n",
    "TEST_IMAGE_PATH = os.path.join(BASE_PATH)\n",
    "TEST_META_PATH = os.path.join(BASE_PATH, 'test.csv')\n",
    " # Path to your saved model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "import cv2 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import joblib # For loading XGB/LGBM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_PATH = '/kaggle/input/dinov2/pytorch/giant/1' \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- LOAD DINO MODEL ---\n",
    "print(\"Loading DINOv2...\")\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_PATH)\n",
    "dino_model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "dino_model.to(device)\n",
    "dino_model.eval()\n",
    "\n",
    "# --- LOAD ML MODELS ---\n",
    "print(\"Loading Ensemble Models...\")\n",
    "# Make sure these paths match where you saved them\n",
    "xgb_model = joblib.load(\"/kaggle/input/baseline-dino-v3/models/lightgbm_ensemble.pkl\")\n",
    "lgb_model = joblib.load(\"/kaggle/input/baseline-dino-v3/models/xgboost_ensemble.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e906d8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:40:25.231693Z",
     "iopub.status.busy": "2025-11-19T11:40:25.230708Z",
     "iopub.status.idle": "2025-11-19T11:40:25.236469Z",
     "shell.execute_reply": "2025-11-19T11:40:25.235531Z"
    },
    "papermill": {
     "duration": 0.012321,
     "end_time": "2025-11-19T11:40:25.238272",
     "exception": false,
     "start_time": "2025-11-19T11:40:25.225951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- !! IMPORTANT: Use the CORRECT target columns !! ---\n",
    "# These names are used to create the final submission file\n",
    "TARGET_COLS =['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "IMG_SIZE = 256 # Must be the same size you trained with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2723e885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:40:25.247679Z",
     "iopub.status.busy": "2025-11-19T11:40:25.247344Z",
     "iopub.status.idle": "2025-11-19T11:40:25.254628Z",
     "shell.execute_reply": "2025-11-19T11:40:25.253679Z"
    },
    "papermill": {
     "duration": 0.014009,
     "end_time": "2025-11-19T11:40:25.256488",
     "exception": false,
     "start_time": "2025-11-19T11:40:25.242479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BiomassTestDataset(Dataset):\n",
    "    def __init__(self, df, base_path, processor, model):\n",
    "        self.df = df\n",
    "        self.image_paths = [os.path.join(base_path, p) for p in df['image_path']]\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.device = model.device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        # 1. Load Image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 2. Process Image for DINO\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        # 3. Extract Features\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Get the pooler_output and move to CPU immediately\n",
    "            # Output shape from DINO is [1, 384]\n",
    "            features = outputs.pooler_output.cpu()\n",
    "            \n",
    "        # Return the flattened feature vector [384,]\n",
    "        return features.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72269f8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-19T11:40:25.265963Z",
     "iopub.status.busy": "2025-11-19T11:40:25.265604Z",
     "iopub.status.idle": "2025-11-19T11:41:19.145245Z",
     "shell.execute_reply": "2025-11-19T11:41:19.141047Z"
    },
    "papermill": {
     "duration": 53.888685,
     "end_time": "2025-11-19T11:41:19.148899",
     "exception": false,
     "start_time": "2025-11-19T11:40:25.260214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from Test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 1/1 [00:53<00:00, 53.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Feature Shape: (5, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare DataLoader ---\n",
    "test_df = pd.read_csv(TEST_META_PATH)\n",
    "\n",
    "test_dataset = BiomassTestDataset(\n",
    "    df=test_df,\n",
    "    base_path=TEST_IMAGE_PATH,\n",
    "    processor=processor,\n",
    "    model=dino_model\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32, \n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# --- Run Feature Extraction ---\n",
    "all_features = []\n",
    "\n",
    "print(\"Extracting features from Test set...\")\n",
    "with torch.no_grad():\n",
    "    for feature_batch in tqdm(test_loader, desc=\"Extracting\"):\n",
    "        # feature_batch shape is [32, 384]\n",
    "        all_features.append(feature_batch.numpy())\n",
    "\n",
    "# Create the Final Feature Matrix\n",
    "X_test = np.vstack(all_features)\n",
    "print(f\"Test Feature Shape: {X_test.shape}\") # Should be (N_test_images, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aefbcf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:41:19.162953Z",
     "iopub.status.busy": "2025-11-19T11:41:19.162337Z",
     "iopub.status.idle": "2025-11-19T11:41:19.172865Z",
     "shell.execute_reply": "2025-11-19T11:41:19.170704Z"
    },
    "papermill": {
     "duration": 0.021887,
     "end_time": "2025-11-19T11:41:19.175310",
     "exception": false,
     "start_time": "2025-11-19T11:41:19.153423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Predicting with Ensemble...\")\n",
    "\n",
    "# # 1. Predict with XGBoost\n",
    "# preds_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# # 2. Predict with LightGBM\n",
    "# preds_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# # 3. Weighted Average (50/50)\n",
    "# ensemble_preds = (0.5 * preds_xgb) + (0.5 * preds_lgb)\n",
    "\n",
    "# # 4. Clip negative values (Biomass cannot be < 0)\n",
    "# ensemble_preds = np.maximum(ensemble_preds, 0)\n",
    "\n",
    "# print(f\"Final Predictions Shape: {ensemble_preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "018a391c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:41:19.186217Z",
     "iopub.status.busy": "2025-11-19T11:41:19.185794Z",
     "iopub.status.idle": "2025-11-19T11:41:19.194970Z",
     "shell.execute_reply": "2025-11-19T11:41:19.192916Z"
    },
    "papermill": {
     "duration": 0.018565,
     "end_time": "2025-11-19T11:41:19.198926",
     "exception": false,
     "start_time": "2025-11-19T11:41:19.180361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_TARGETS=len(TARGET_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "809616e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:41:19.212610Z",
     "iopub.status.busy": "2025-11-19T11:41:19.212249Z",
     "iopub.status.idle": "2025-11-19T11:41:19.218743Z",
     "shell.execute_reply": "2025-11-19T11:41:19.217359Z"
    },
    "papermill": {
     "duration": 0.017757,
     "end_time": "2025-11-19T11:41:19.221760",
     "exception": false,
     "start_time": "2025-11-19T11:41:19.204003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "META_LEARNER_COEFFS = np.array([\n",
    "    [1.267, -0.217],  # Target 1\n",
    "    [0.686, 0.262],   # Target 2\n",
    "    [0.985, 0.211],   # Target 3\n",
    "    [1.137, 0.003],   # Target 4\n",
    "    [0.822, 0.372]    # Target 5\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11af8cd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:41:19.232660Z",
     "iopub.status.busy": "2025-11-19T11:41:19.232135Z",
     "iopub.status.idle": "2025-11-19T11:41:19.276946Z",
     "shell.execute_reply": "2025-11-19T11:41:19.275769Z"
    },
    "papermill": {
     "duration": 0.053226,
     "end_time": "2025-11-19T11:41:19.279378",
     "exception": false,
     "start_time": "2025-11-19T11:41:19.226152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting with Optimal Ensemble Blend...\n",
      "Final Predictions Shape: (5, 5)\n",
      "\n",
      "Creating submission file...\n"
     ]
    }
   ],
   "source": [
    "# --- PREDICTION STEP: APPLYING OPTIMAL WEIGHTS ---\n",
    "\n",
    "print(\"\\nPredicting with Optimal Ensemble Blend...\")\n",
    "\n",
    "# 1. Predict with XGBoost\n",
    "preds_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# 2. Predict with LightGBM\n",
    "preds_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# 3. Optimal Weighted Average (Target-Specific Stacking)\n",
    "ensemble_preds_optimal = np.zeros_like(preds_xgb)\n",
    "\n",
    "# Apply the learned optimal weights (coefficients) column by column\n",
    "for i in range(N_TARGETS):\n",
    "    xgb_weight = META_LEARNER_COEFFS[i, 0]\n",
    "    lgbm_weight = META_LEARNER_COEFFS[i, 1]\n",
    "    \n",
    "    # Blend the predictions for the current target (column i)\n",
    "    ensemble_preds_optimal[:, i] = (\n",
    "        xgb_weight * preds_xgb[:, i] + \n",
    "        lgbm_weight * preds_lgb[:, i]\n",
    "    )\n",
    "\n",
    "# 4. Clip negative values (Biomass cannot be < 0)\n",
    "ensemble_preds = np.maximum(ensemble_preds_optimal, 0)\n",
    "\n",
    "print(f\"Final Predictions Shape: {ensemble_preds.shape}\")\n",
    "# --- Create Submission File ---\n",
    "print(\"\\nCreating submission file...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8cd0284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T11:41:19.290462Z",
     "iopub.status.busy": "2025-11-19T11:41:19.290040Z",
     "iopub.status.idle": "2025-11-19T11:41:19.357789Z",
     "shell.execute_reply": "2025-11-19T11:41:19.355965Z"
    },
    "papermill": {
     "duration": 0.076425,
     "end_time": "2025-11-19T11:41:19.360423",
     "exception": false,
     "start_time": "2025-11-19T11:41:19.283998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file...\n",
      "submission.csv created successfully!\n",
      "                     sample_id     target\n",
      "0   ID1001187975__Dry_Clover_g   3.474077\n",
      "5     ID1001187975__Dry_Dead_g  19.054260\n",
      "10   ID1001187975__Dry_Green_g  24.684383\n",
      "15   ID1001187975__Dry_Total_g  57.410470\n",
      "20         ID1001187975__GDM_g  28.693285\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Create Submission File ---\n",
    "print(\"Creating submission file...\")\n",
    "\n",
    "# 1. Create a \"wide\" DataFrame with predictions\n",
    "pred_df = pd.DataFrame(ensemble_preds, columns=TARGET_COLS)\n",
    "\n",
    "# 2. Add the 'image_path' from the original test_df\n",
    "submission_df = test_df[['image_path']].copy()\n",
    "submission_df = pd.concat([submission_df, pred_df], axis=1)\n",
    "\n",
    "# 3. \"Melt\" the DataFrame from \"wide\" to \"long\" format\n",
    "submission_df_long = submission_df.melt(\n",
    "    id_vars=['image_path'],\n",
    "    value_vars=TARGET_COLS,\n",
    "    var_name='target_name',\n",
    "    value_name='target'\n",
    ")\n",
    "\n",
    "# 4. Create the 'sample_id'\n",
    "# Clean up the filename (remove path and extension)\n",
    "submission_df_long['image_name_base'] = submission_df_long['image_path'].apply(\n",
    "    lambda p: os.path.splitext(os.path.basename(p))[0]\n",
    ")\n",
    "\n",
    "# Create ID: image_base + __ + target_name\n",
    "submission_df_long['sample_id'] = submission_df_long['image_name_base'] + '__' + submission_df_long['target_name']\n",
    "\n",
    "# 5. Filter duplicates and Select Columns\n",
    "final_submission = submission_df_long[['sample_id', 'target']]\n",
    "final_submission = final_submission.drop_duplicates(subset=['sample_id'])\n",
    "\n",
    "# 6. Save\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"submission.csv created successfully!\")\n",
    "print(final_submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a70acb3",
   "metadata": {
    "papermill": {
     "duration": 0.005312,
     "end_time": "2025-11-19T11:41:19.370039",
     "exception": false,
     "start_time": "2025-11-19T11:41:19.364727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "isSourceIdPinned": false,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "datasetId": 8768931,
     "sourceId": 13776976,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 277095319,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 279877522,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 986,
     "modelInstanceId": 3329,
     "sourceId": 4537,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 162.709734,
   "end_time": "2025-11-19T11:41:22.315332",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-19T11:38:39.605598",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
